---
title: senses!
---
<div class="works">


	<div class="welcome">
		<h3>Seeing With Touch</h3>
		<h6>
			This concept reinterprets vision by remapping touch information onto paper, allowing you to "see" beyond your physical and sensory body.  (a live transcribing brail device)
		</h6>
	</div>

	<div class="desc">
		<div class="third">
			<h5>duration</h5>
			<p id="one">Sept - Nov 2017 / 7 weeks</p>
		</div>
		<div class="third">
			<h5>role / skills</h5>
			<p>Individual Studio Project for Arch 1101; Study Modeling / Freehand Drawing / Concept Development / Ergonomics / Fabrication</p>
		</div>
		<div class="third">
			<h5>tools</h5>
			<p>Pencil & Paper, Machining, Brazing, Photoshop, Sketchup</p>
		</div>
	</div>

	<div class="container" id="x">
		<img id="full" img src="/images/xnormal/touch-cover.jpg"></a>
	</div>

	<!--<div class="video-container">
		<iframe width="642" height="350" src="https://www.youtube.com/embed/VRvk01TDM7I" frameborder="0" allowfullscreen></iframe>
	</div>-->

	<div class="text">
		<h5>The Inspiration</h5>
		<p>I began my research on the senses (touch, sight, smell, taste, sound) with an animal that exemplified one: the star-nosed mole and its hypersensitive touch. I researched how the nearly-blind animal navigates solely with its sense of touch. The star-nosed mole “sees” by collecting thousands of data points with its nose-like touch receptor, sewing together the touch information instantaneously to create a mental sonogram of its environment. All without using its eyes.
		</p>
	</div>

	<div class="container">
		<a data-fancybox="gallery" data-caption="" href="/images/large/mole.jpg"><img src="/images/normal/mole.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/mole2.jpg"><img src="/images/normal/mole2.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/mole-diagram.jpg"><img id="full" src="/images/normal/mole-diagram.jpg"></a>
	</div>

	<div class="text">
		<h5>Ideation</h5>
		<p>My drawings explored the anatomy and function of the star-nosed mole. My study models honed in on the relationship between touch and sight. Humans use sight as the primary means to understand the world. How could I capture the nuance of touch with the same gradation as human sight?
		</p>
	</div>

	<div class="container">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-3.jpg"><img src="/images/normal/p2-3.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-4.jpg"><img src="/images/normal/p2-4.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-2.jpg"><img src="/images/normal/p2-2.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-6.jpg"><img src="/images/normal/p2-6.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-5.jpg"><img src="/images/normal/p2-5.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-1.jpg"><img src="/images/normal/p2-1.jpg"></a>
	</div>

	<div class="text">
		<h5>Synthesis</h5>
		<p>I became fascinated with the complexity of translating tactile sensation into sight information. The end goal of the project was to create a sensory augmentation device that would enhance the human’s sense of touch. I devised a method of displaying touch information visually. The method would simulate what it would be like to literally <i>see</i> with touch.
		</p>
	</div>

	<div class="container">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-11.jpg"><img src="/images/normal/p2-11.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-12.jpg"><img src="/images/normal/p2-12.jpg"></a>
	</div>

	<div class="text">
		<h5>Prototype No.1: Translating Touch to Sight</h5>
		<p>Proximate objects are plotted in the viewfinder. The device has thirty-six nodes of resolution, with each node acting like a pixel on a screen– a representation of the external environment condition at a specific point in space.
		</p>
	</div>

	<div class="container">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-23.jpg"><img src="/images/normal/p2-23.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-22.jpg"><img src="/images/normal/p2-22.jpg"></a>
	</div>

	<div class="text">
		<h5>How it Works</h5>
		<p>Three-foot steel wires form a rectangular grid at regular intervals. This grid is the viewing area of the device. Each steel wire acts as an antenna– when the end of the wire contacts an external stimulus, the wire compresses and moves an indicator at the opposite end. The indicator presses against the opaque viewfinder of the device– the user can see the compressed node but all else is obscured. In this way, the viewer walks around the environment, brushing the steel wires against surface conditions. As different nodes are activated, the user can piece together touch information about the environment.
		</p>
	</div>
	<div class="container">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-21.jpg"><img id="full" src="/images/normal/p2-21.jpg"></a>
	</div>

	<div class="text">
		<h5>Notes from Prototype No.1</h5>
		<p>The steel piano wire proved to be too flexible, bending instead of sliding within the frame. And the wooden frame was prone to breaking due to the high stress on the wire supports.
		</p>
		<p>Ergonomics is also important and overlooked in the first prototype. The head and body of the user are the site of the device. For the first iteration, weight was not much of an issue, but with a sturdier build for the final machine, face and arm supports are necessary to accommodate the cantilevered steel mass.
		</p>
	</div>

	<div class="container" id="minimize">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-axon.jpg"><img id="full" src="/images/normal/p2-axon.jpg"></a>
	</div>

	<div class="text"></div>
	<div class="text">
		<h5>The Final Machine</h5>
		<p>The final machine is an evolution of the previous device in two ways: an all-steel construction allowing for higher precision, and an archival addition that records the environment conditions. For the archival aspect, I had to figure out a way to transfer touch information onto something that can be reread. Through experimentation, ink on paper was the most reliable way for transcription. I tested a number of rubber "feet" designs that change shape depending on the pressure applied to the steel rod.
		</p>
	</div>

	<div class="container">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-33.jpg"><img src="/images/normal/p2-33.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-34.jpg"><img src="/images/normal/p2-34.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-31.jpg"><img src="/images/normal/p2-31.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-32.jpg"><img src="/images/normal/p2-32.jpg"></a>
	</div>

	<div class="text">
		<p>
			The machine allows you to “see” beyond the scope of your physical and sensory capabilities. Each of the twenty-four rods are designed to transfer ink onto a roll of paper when pushed from the opposite end. A small electric motor feeds the roll of paper through the machine at a slow, constant rate, recording time on the Y axis. As the user walks around the environment, the rods transfer information of surface conditions to the paper, and the user can “see” the translated conditions as ink marks from behind the paper.
		</p>
	</div>

	<div class="container" id="minimize">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-58.jpg"><img id="full" src="/images/normal/p2-58.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-51.jpg"><img id="full" src="/images/normal/p2-51.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-52.jpg"><img src="/images/normal/p2-52.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-55.jpg"><img src="/images/normal/p2-55.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-57.jpg"><img src="/images/normal/p2-57.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-56.jpg"><img src="/images/normal/p2-56.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-54.jpg"><img src="/images/normal/p2-54.jpg"></a>
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-53.jpg"><img src="/images/normal/p2-53.jpg"></a>
	</div>



	<div class="text">
		<p>
			The topography of the environment becomes a series of irregular marks within
			equally spaced channels. The density, continuity, and weight of the ink changes, but
			they always reside within their respective channel. This false topography is both real
			and imagined, totally honest yet deceitful at the same time.
		</p>
		<p>
			Below is the actual archival drawing created by the machine.
		</p>
	</div>

	<div class="container">
		<a data-fancybox="gallery" data-caption="" href="/images/large/p2-41.jpg"><img id="full" src="/images/normal/p2-41.jpg"></a>
	</div>

</div>
